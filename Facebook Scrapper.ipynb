{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Claritin Facebook Page: 2017-12-03 12:39:52.332826\n",
      "\n",
      "HTTP Error 400: Bad Request\n",
      "Error for URL https://graph.facebook.com/v2.9/Claritin/posts/?limit=100&access_token=1817819741852164|b0ec96124ba0807a6093455a86035cc3&fields=message,link,created_time,type,name,id,comments.limit(0).summary(true),shares,reactions.limit(0).summary(true): 2017-12-03 12:39:57.463507\n",
      "Retrying.\n",
      "HTTP Error 400: Bad Request\n",
      "Error for URL https://graph.facebook.com/v2.9/Claritin/posts/?limit=100&access_token=1817819741852164|b0ec96124ba0807a6093455a86035cc3&fields=message,link,created_time,type,name,id,comments.limit(0).summary(true),shares,reactions.limit(0).summary(true): 2017-12-03 12:40:02.656411\n",
      "Retrying.\n",
      "HTTP Error 400: Bad Request\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a0e80f10b6a0>\u001b[0m in \u001b[0;36mrequest_until_succeed\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    531\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    641\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 642\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    569\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    649\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 400: Bad Request",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a0e80f10b6a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m     \u001b[0mscrapeFacebookPageFeedStatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccess_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msince_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muntil_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-a0e80f10b6a0>\u001b[0m in \u001b[0;36mscrapeFacebookPageFeedStatus\u001b[1;34m(page_id, access_token, since_date, until_date)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetFacebookPageFeedUrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0mstatuses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest_until_succeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m             \u001b[0mreactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetReactionsForStatuses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-a0e80f10b6a0>\u001b[0m in \u001b[0;36mrequest_until_succeed\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error for URL {}: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "try:\n",
    "    from urllib.request import urlopen, Request\n",
    "except ImportError:\n",
    "    from urllib2 import urlopen, Request\n",
    "\n",
    "app_id = \"1817819741852164\"\n",
    "app_secret = \"b0ec96124ba0807a6093455a86035cc3\"  # DO NOT SHARE WITH ANYONE!\n",
    "page_id = \"Claritin\"\n",
    "\n",
    "# input date formatted as YYYY-MM-DD\n",
    "since_date = \"\"\n",
    "until_date = \"\"\n",
    "\n",
    "access_token = app_id + \"|\" + app_secret\n",
    "\n",
    "\n",
    "def request_until_succeed(url):\n",
    "    req = Request(url)\n",
    "    success = False\n",
    "    while success is False:\n",
    "        try:\n",
    "            response = urlopen(req)\n",
    "            if response.getcode() == 200:\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(5)\n",
    "\n",
    "            print(\"Error for URL {}: {}\".format(url, datetime.datetime.now()))\n",
    "            print(\"Retrying.\")\n",
    "\n",
    "    return response.read()\n",
    "\n",
    "\n",
    "# Needed to write tricky unicode correctly to csv\n",
    "def unicode_decode(text):\n",
    "    try:\n",
    "        return text.encode('utf-8').decode()\n",
    "    except UnicodeDecodeError:\n",
    "        return text.encode('utf-8')\n",
    "\n",
    "\n",
    "def getFacebookPageFeedUrl(base_url):\n",
    "\n",
    "    # Construct the URL string; see http://stackoverflow.com/a/37239851 for\n",
    "    # Reactions parameters\n",
    "    fields = \"&fields=message,link,created_time,type,name,id,\" + \\\n",
    "        \"comments.limit(0).summary(true),shares,reactions\" + \\\n",
    "        \".limit(0).summary(true)\"\n",
    "\n",
    "    return base_url + fields\n",
    "\n",
    "\n",
    "def getReactionsForStatuses(base_url):\n",
    "\n",
    "    reaction_types = ['like', 'love', 'wow', 'haha', 'sad', 'angry']\n",
    "    reactions_dict = {}   # dict of {status_id: tuple<6>}\n",
    "\n",
    "    for reaction_type in reaction_types:\n",
    "        fields = \"&fields=reactions.type({}).limit(0).summary(total_count)\".format(\n",
    "            reaction_type.upper())\n",
    "\n",
    "        url = base_url + fields\n",
    "\n",
    "        data = json.loads(request_until_succeed(url))['data']\n",
    "\n",
    "        data_processed = set()  # set() removes rare duplicates in statuses\n",
    "        for status in data:\n",
    "            id = status['id']\n",
    "            count = status['reactions']['summary']['total_count']\n",
    "            data_processed.add((id, count))\n",
    "\n",
    "        for id, count in data_processed:\n",
    "            if id in reactions_dict:\n",
    "                reactions_dict[id] = reactions_dict[id] + (count,)\n",
    "            else:\n",
    "                reactions_dict[id] = (count,)\n",
    "\n",
    "    return reactions_dict\n",
    "\n",
    "\n",
    "def processFacebookPageFeedStatus(status):\n",
    "\n",
    "    # The status is now a Python dictionary, so for top-level items,\n",
    "    # we can simply call the key.\n",
    "\n",
    "    # Additionally, some items may not always exist,\n",
    "    # so must check for existence first\n",
    "\n",
    "    status_id = status['id']\n",
    "    status_type = status['type']\n",
    "\n",
    "    status_message = '' if 'message' not in status else \\\n",
    "        unicode_decode(status['message'])\n",
    "    link_name = '' if 'name' not in status else \\\n",
    "        unicode_decode(status['name'])\n",
    "    status_link = '' if 'link' not in status else \\\n",
    "        unicode_decode(status['link'])\n",
    "\n",
    "    # Time needs special care since a) it's in UTC and\n",
    "    # b) it's not easy to use in statistical programs.\n",
    "\n",
    "    status_published = datetime.datetime.strptime(\n",
    "        status['created_time'], '%Y-%m-%dT%H:%M:%S+0000')\n",
    "    status_published = status_published + \\\n",
    "        datetime.timedelta(hours=-5)  # EST\n",
    "    status_published = status_published.strftime(\n",
    "        '%Y-%m-%d %H:%M:%S')  # best time format for spreadsheet programs\n",
    "\n",
    "    # Nested items require chaining dictionary keys.\n",
    "\n",
    "    num_reactions = 0 if 'reactions' not in status else \\\n",
    "        status['reactions']['summary']['total_count']\n",
    "    num_comments = 0 if 'comments' not in status else \\\n",
    "        status['comments']['summary']['total_count']\n",
    "    num_shares = 0 if 'shares' not in status else status['shares']['count']\n",
    "\n",
    "    return (status_id, status_message, link_name, status_type, status_link,\n",
    "            status_published, num_reactions, num_comments, num_shares)\n",
    "\n",
    "\n",
    "def scrapeFacebookPageFeedStatus(page_id, access_token, since_date, until_date):\n",
    "    with open('{}_facebook_statuses.csv'.format(page_id), 'w') as file:\n",
    "        w = csv.writer(file)\n",
    "        w.writerow([\"status_id\", \"status_message\", \"link_name\", \"status_type\",\n",
    "                    \"status_link\", \"status_published\", \"num_reactions\",\n",
    "                    \"num_comments\", \"num_shares\", \"num_likes\", \"num_loves\",\n",
    "                    \"num_wows\", \"num_hahas\", \"num_sads\", \"num_angrys\",\n",
    "                    \"num_special\"])\n",
    "\n",
    "        has_next_page = True\n",
    "        num_processed = 0\n",
    "        scrape_starttime = datetime.datetime.now()\n",
    "        after = ''\n",
    "        base = \"https://graph.facebook.com/v2.9\"\n",
    "        node = \"/{}/posts\".format(page_id)\n",
    "        parameters = \"/?limit={}&access_token={}\".format(100, access_token)\n",
    "        since = \"&since={}\".format(since_date) if since_date \\\n",
    "            is not '' else ''\n",
    "        until = \"&until={}\".format(until_date) if until_date \\\n",
    "            is not '' else ''\n",
    "\n",
    "        print(\"Scraping {} Facebook Page: {}\\n\".format(page_id, scrape_starttime))\n",
    "\n",
    "        while has_next_page:\n",
    "            after = '' if after is '' else \"&after={}\".format(after)\n",
    "            base_url = base + node + parameters + after + since + until\n",
    "\n",
    "            url = getFacebookPageFeedUrl(base_url)\n",
    "            statuses = json.loads(request_until_succeed(url))\n",
    "            reactions = getReactionsForStatuses(base_url)\n",
    "\n",
    "            for status in statuses['data']:\n",
    "\n",
    "                # Ensure it is a status with the expected metadata\n",
    "                if 'reactions' in status:\n",
    "                    status_data = processFacebookPageFeedStatus(status)\n",
    "                    reactions_data = reactions[status_data[0]]\n",
    "\n",
    "                    # calculate thankful/pride through algebra\n",
    "                    num_special = status_data[6] - sum(reactions_data)\n",
    "                    try:\n",
    "                        w.writerow(status_data + reactions_data + (num_special,))\n",
    "                    except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "                        print (\"Caught unicode error\")\n",
    "                   \n",
    "\n",
    "                num_processed += 1\n",
    "                if num_processed % 100 == 0:\n",
    "                    print(\"{} Statuses Processed: {}\".format\n",
    "                          (num_processed, datetime.datetime.now()))\n",
    "\n",
    "            # if there is no next page, we're done.\n",
    "            if 'paging' in statuses:\n",
    "                after = statuses['paging']['cursors']['after']\n",
    "            else:\n",
    "                has_next_page = False\n",
    "\n",
    "        print(\"\\nDone!\\n{} Statuses Processed in {}\".format(\n",
    "              num_processed, datetime.datetime.now() - scrape_starttime))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scrapeFacebookPageFeedStatus(page_id, access_token, since_date, until_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
